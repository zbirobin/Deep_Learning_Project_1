{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f822ed68324f9bbffbaab1f73baed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752ccd5169464bd5a761579ccd69218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a1e31c943849ee9e0617c3f2c23098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ee977710af46b58b62832f9df2af0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9, 3],\n",
       "        [5, 4],\n",
       "        [7, 4],\n",
       "        [9, 6],\n",
       "        [8, 8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000 # Number of data samples in training and test set\n",
    "\n",
    "train_input, train_target, train_classes, \\\n",
    "    test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(train_classes.shape)\n",
    "\n",
    "train_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans les practical il prend le mean de tout le input\n",
    "# pas sur que ce soit le mieux, a changer peut etre\n",
    "def normalize(input, mean, std):\n",
    "    input.sub_(mean).div_(std)\n",
    "  \n",
    "mean = train_input.mean()\n",
    "std = train_input.std()\n",
    "\n",
    "normalize(train_input, mean, std)\n",
    "normalize(test_input, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(DigitNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNetSingleOutput(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(DigitNetSingleOutput, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pairInputModel(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(pairInputModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size=25, \n",
    "                nb_epochs=25, criterion=nn.CrossEntropyLoss(), lr=1e-1):\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to train a siamese model.\n",
    "Since this is used to compare two images as input, we use the contrastive loss rather than the cross entropy.\n",
    "'''\n",
    "def train_siamese_model(model, train_input_1, train_input_2, train_target, mini_batch_size=25, \n",
    "                nb_epochs=25, criterion=nn.CrossEntropyLoss(), lr=1e-1):\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            # Forward pass\n",
    "            features_1 = model(train_input_1.narrow(0, b, mini_batch_size))\n",
    "            features_2 = model(train_input_2.narrow(0, b, mini_batch_size))\n",
    "            batch_target = train_target.narrow(0, b, mini_batch_size)\n",
    "            \n",
    "            # Compute the contrastive loss\n",
    "            #euclidean_distance = F.pairwise_distance(features_1, features_2)\n",
    "            #loss_contrastive = torch.mean((1 - batch_target) * torch.pow(euclidean_distance, 2) +\n",
    "            #                          batch_target * torch.pow(torch.clamp(2 - euclidean_distance, min=0.0), 2))\n",
    "            \n",
    "            loss = criterion(torch.cat((features_1, features_2), 1), batch_target)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 14, 14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_flat = train_input[:,0,:,:].reshape(N, 1, 14, 14)\n",
    "test_input_flat = test_input[:,0,:,:].reshape(N, 1, 14, 14)\n",
    "train_input_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes_flat = train_classes[:,0].reshape(-1)\n",
    "test_classes_flat = test_classes[:,0].reshape(-1)\n",
    "test_classes_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size=25):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model, data_input_1, data_input_2, data_target, mini_batch_size=25):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input_1.size(0), mini_batch_size):\n",
    "        output_1 = model(data_input_1.narrow(0, b, mini_batch_size))\n",
    "        output_2 = model(data_input_2.narrow(0, b, mini_batch_size))\n",
    "        output = torch.cat((output_1, output_2), 1)\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error(model, tr_input, tr_target, te_input, te_target):\n",
    "    print('train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "                compute_nb_errors(model, tr_input, tr_target) / N * 100,\n",
    "                compute_nb_errors(model, te_input, te_target) / N * 100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_siamese(model, tr_input_1, tr_input_2, tr_target, te_input_1, te_input_2, te_target):\n",
    "    print('train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "                compute_nb_errors_siamese(model, tr_input_1, tr_input_2, tr_target) / N * 100,\n",
    "                compute_nb_errors_siamese(model, te_input_1, te_input_2, te_target) / N * 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitNet(500)\n",
    "\n",
    "train_model(model, train_input_flat, train_classes_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error 0.00% test_error 5.70%\n"
     ]
    }
   ],
   "source": [
    "print_error(model, train_input_flat, train_classes_flat, test_input_flat, test_classes_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pairInputModel(500)\n",
    "train_model(model, train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error 0.00% test_error 18.00%\n"
     ]
    }
   ],
   "source": [
    "print_error(model, train_input, train_target, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152326"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_1 = train_input[:,0,:,:].reshape(N, 1, 14, 14)\n",
    "train_input_2 = train_input[:,1,:,:].reshape(N, 1, 14, 14)\n",
    "\n",
    "test_input_1 = test_input[:,0,:,:].reshape(N, 1, 14, 14)\n",
    "test_input_2 = test_input[:,1,:,:].reshape(N, 1, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitNetSingleOutput(500)\n",
    "train_siamese_model(model, train_input_1, train_input_2, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error 0.00% test_error 13.30%\n"
     ]
    }
   ],
   "source": [
    "print_error_siamese(model, train_input_1, train_input_2, train_target, test_input_1, test_input_2, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
