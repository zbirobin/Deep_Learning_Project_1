{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(Tensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_t = Tensor([[0],[1],[1],[0],[1]])\n",
    "print(index_t.shape)\n",
    "def encode_targets(target):\n",
    "    n = target.size(0)\n",
    "    result = torch.zeros((n,2))\n",
    "    return result.scatter(1,target.reshape(n,1).long(),1)\n",
    "#test\n",
    "print(encode_targets(index_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7],\n",
       "        [2, 4],\n",
       "        [5, 3],\n",
       "        [4, 9],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000 # Number of data samples in training and test set\n",
    "\n",
    "train_input, train_target, train_classes, \\\n",
    "    test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "# use 1-hot encoding for targets\n",
    "train_target = encode_targets(train_target)\n",
    "test_target = encode_targets(test_target)\n",
    "\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(train_classes.shape)\n",
    "\n",
    "train_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input, mean, std):\n",
    "    input.sub_(mean).div_(std)\n",
    "    \n",
    "def process_data(img_input, classes, one_hot_classes=False):\n",
    "    \n",
    "    n_img = img_input.size(0) \n",
    "    img_input_1 = img_input[:,0,:,:].reshape(n_img, 1, 14, 14)\n",
    "    img_input_2 = img_input[:,1,:,:].reshape(n_img, 1, 14, 14)\n",
    "    \n",
    "    img_classes_1 = prologue.convert_to_one_hot_labels(img_input_1, classes[:,0]) if one_hot_classes else classes[:,0]\n",
    "    img_classes_2 = prologue.convert_to_one_hot_labels(img_input_2, classes[:,1]) if one_hot_classes else classes[:,1]\n",
    "    \n",
    "    img_classes_1.reshape(-1,1)\n",
    "    img_classes_2.reshape(-1,1)\n",
    "    \n",
    "    return img_input_1, img_input_2, img_classes_1, img_classes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_input.mean(dim=(0,2,3), keepdim=True)\n",
    "std = train_input.std(dim=(0,2,3), keepdim=True)\n",
    "\n",
    "normalize(train_input, mean, std)\n",
    "normalize(test_input, mean, std)\n",
    "\n",
    "train_input_1, train_input_2, train_classes_1, train_classes_2 = process_data(train_input, train_classes)\n",
    "test_input_1, test_input_2, test_classes_1, test_classes_2 = process_data(test_input, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(DigitNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompNet(torch.nn.Module):\n",
    "    def __init__(self, digitNet):\n",
    "        super(CompNet, self).__init__()\n",
    "        self.digitNet = digitNet\n",
    "        self.fc1 = nn.Linear(20, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "        \n",
    "    def forward(self, x1, x2,train=True):\n",
    "        x1 = self.digitNet.forward(x1)\n",
    "        x2 = self.digitNet.forward(x2)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x,p=0.25,training=train)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x,p=0.25,training=train)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        ...,\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "train_target = encode_targets(train_target)\n",
    "print(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = encode_targets(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_digit, model_comp, \n",
    "                train_input_1, train_input_2, train_classes_1, train_classes_2, train_target, \n",
    "                criterion_digit=nn.CrossEntropyLoss(), criterion_comp=nn.BCELoss(), \n",
    "                mini_batch_size=25,nb_epochs=50, lr=1e-1):\n",
    "    \n",
    "    optimizer_comp = torch.optim.SGD(model_comp.parameters(), lr=lr)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        if e % 5 == 0:\n",
    "            print(\"Epochs {}\".format(e))\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            # digit classification \n",
    "            output_img_1 = model_digit(train_input_1.narrow(0, b, mini_batch_size))\n",
    "            output_img_2 = model_digit(train_input_2.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            loss_img_1 = criterion_digit(output_img_1, train_classes_1.narrow(0, b, mini_batch_size))\n",
    "            loss_img_2 = criterion_digit(output_img_2, train_classes_2.narrow(0, b, mini_batch_size))\n",
    "            loss_img = loss_img_1 + loss_img_2\n",
    "            \n",
    "            output_comp = model_comp(train_input_1.narrow(0, b, mini_batch_size), train_input_2.narrow(0, b, mini_batch_size))\n",
    "            #batch_target = train_target.narrow(0, b, mini_batch_size).reshape(-1,1).float()\n",
    "            batch_target = train_target.narrow(0,b,mini_batch_size)\n",
    "            #print(batch_target)\n",
    "            #print(output_comp)\n",
    "            loss_comp = criterion_comp(output_comp, batch_target)\n",
    "            \n",
    "            loss = loss_img + loss_comp\n",
    "            \n",
    "            if b==0:\n",
    "                print(\"loss = {}, loss_img = {}, loss_comp = {}\".format(loss, loss_img, loss_comp))\n",
    "                \n",
    "            model_digit.zero_grad()\n",
    "            model_comp.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_comp.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152326\n",
      "156028\n",
      "training...\n",
      "Epochs 0\n",
      "loss = 5.314850330352783, loss_img = 4.6233930587768555, loss_comp = 0.6914573907852173\n",
      "loss = 2.118473768234253, loss_img = 1.4228087663650513, loss_comp = 0.6956650614738464\n",
      "loss = 2.359877347946167, loss_img = 1.7132670879364014, loss_comp = 0.6466102004051208\n",
      "loss = 1.2319447994232178, loss_img = 0.648379921913147, loss_comp = 0.583564817905426\n",
      "loss = 0.7087023854255676, loss_img = 0.2256259024143219, loss_comp = 0.4830764830112457\n",
      "Epochs 5\n",
      "loss = 0.6012747883796692, loss_img = 0.08515288680791855, loss_comp = 0.5161219239234924\n",
      "loss = 0.4575989544391632, loss_img = 0.058027006685733795, loss_comp = 0.39957195520401\n",
      "loss = 0.4420558214187622, loss_img = 0.034904494881629944, loss_comp = 0.40715134143829346\n",
      "loss = 0.48331746459007263, loss_img = 0.06482523679733276, loss_comp = 0.41849222779273987\n",
      "loss = 0.41554391384124756, loss_img = 0.02033272013068199, loss_comp = 0.39521118998527527\n",
      "Epochs 10\n",
      "loss = 0.41421470046043396, loss_img = 0.009891796857118607, loss_comp = 0.40432289242744446\n",
      "loss = 0.3826938569545746, loss_img = 0.011766050010919571, loss_comp = 0.3709278106689453\n",
      "loss = 0.36623671650886536, loss_img = 0.004974540323019028, loss_comp = 0.36126217246055603\n",
      "loss = 0.3727884888648987, loss_img = 0.0035715841222554445, loss_comp = 0.3692169189453125\n",
      "loss = 0.3715737760066986, loss_img = 0.0030909155029803514, loss_comp = 0.3684828579425812\n",
      "Epochs 15\n",
      "loss = 0.3927636742591858, loss_img = 0.003751943353563547, loss_comp = 0.3890117406845093\n",
      "loss = 0.4163708984851837, loss_img = 0.0025069480761885643, loss_comp = 0.4138639569282532\n",
      "loss = 0.40063413977622986, loss_img = 0.0034015197306871414, loss_comp = 0.39723262190818787\n",
      "loss = 0.353189617395401, loss_img = 0.0020118444226682186, loss_comp = 0.35117778182029724\n",
      "loss = 0.35336998105049133, loss_img = 0.002709532855078578, loss_comp = 0.35066044330596924\n",
      "Epochs 20\n",
      "loss = 0.3645586669445038, loss_img = 0.001615175511687994, loss_comp = 0.36294350028038025\n",
      "loss = 4.969475746154785, loss_img = 4.306613922119141, loss_comp = 0.6628616452217102\n",
      "loss = 2.6923487186431885, loss_img = 2.1295652389526367, loss_comp = 0.5627834796905518\n",
      "loss = 1.0919203758239746, loss_img = 0.6350929141044617, loss_comp = 0.4568275213241577\n",
      "loss = 0.8763753175735474, loss_img = 0.4090691804885864, loss_comp = 0.46730613708496094\n",
      "Epochs 25\n",
      "loss = 0.7035967111587524, loss_img = 0.28708016872406006, loss_comp = 0.4165165424346924\n",
      "loss = 0.6611080169677734, loss_img = 0.23290768265724182, loss_comp = 0.42820030450820923\n",
      "loss = 0.5996813774108887, loss_img = 0.1772347390651703, loss_comp = 0.4224466383457184\n",
      "loss = 0.5039750337600708, loss_img = 0.11579808592796326, loss_comp = 0.38817691802978516\n",
      "loss = 0.4421902894973755, loss_img = 0.06998142600059509, loss_comp = 0.3722088634967804\n",
      "Epochs 30\n",
      "loss = 0.40386009216308594, loss_img = 0.043569114059209824, loss_comp = 0.3602909743785858\n",
      "loss = 0.41132864356040955, loss_img = 0.02185993082821369, loss_comp = 0.3894686996936798\n",
      "loss = 1.5172169208526611, loss_img = 1.0398352146148682, loss_comp = 0.47738173604011536\n",
      "loss = 0.8243519067764282, loss_img = 0.3872433304786682, loss_comp = 0.4371085464954376\n",
      "loss = 0.5965296030044556, loss_img = 0.2072085738182068, loss_comp = 0.3893210291862488\n",
      "Epochs 35\n",
      "loss = 0.5779225826263428, loss_img = 0.16281795501708984, loss_comp = 0.41510462760925293\n",
      "loss = 0.46510785818099976, loss_img = 0.09944367408752441, loss_comp = 0.36566418409347534\n",
      "loss = 0.46450474858283997, loss_img = 0.08388432115316391, loss_comp = 0.38062041997909546\n",
      "loss = 0.42972707748413086, loss_img = 0.03900571167469025, loss_comp = 0.3907213509082794\n",
      "loss = 0.378726065158844, loss_img = 0.02530159056186676, loss_comp = 0.3534244894981384\n",
      "Epochs 40\n",
      "loss = 0.3863909840583801, loss_img = 0.026640068739652634, loss_comp = 0.3597509264945984\n",
      "loss = 0.3683391213417053, loss_img = 0.01323887798935175, loss_comp = 0.35510024428367615\n",
      "loss = 0.3647693991661072, loss_img = 0.014715993776917458, loss_comp = 0.3500533998012543\n",
      "loss = 0.3746168613433838, loss_img = 0.024304643273353577, loss_comp = 0.3503122329711914\n",
      "loss = 0.35586848855018616, loss_img = 0.006828716024756432, loss_comp = 0.349039763212204\n",
      "Epochs 45\n",
      "loss = 0.35932740569114685, loss_img = 0.0067903511226177216, loss_comp = 0.3525370657444\n",
      "loss = 0.35720184445381165, loss_img = 0.006382419262081385, loss_comp = 0.3508194386959076\n",
      "loss = 0.37486958503723145, loss_img = 0.024185702204704285, loss_comp = 0.35068386793136597\n",
      "loss = 0.3609994947910309, loss_img = 0.013841353356838226, loss_comp = 0.34715813398361206\n",
      "loss = 0.3612993657588959, loss_img = 0.006241285242140293, loss_comp = 0.35505807399749756\n"
     ]
    }
   ],
   "source": [
    "model_digit = DigitNet(500)\n",
    "model_comp = CompNet(model_digit)\n",
    "\n",
    "print(sum(p.numel() for p in model_digit.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in model_comp.parameters() if p.requires_grad))\n",
    "print(\"training...\")\n",
    "\n",
    "train_model(model_digit=model_digit, model_comp=model_comp,\n",
    "            train_input_1=train_input_1, train_input_2=train_input_2,\n",
    "            train_classes_1=train_classes_1, train_classes_2=train_classes_2, \n",
    "            train_target=train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model_digit, model_comp,\n",
    "                              data_input_1, data_input_2, data_target, mini_batch_size=25):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input_1.size(0), mini_batch_size):\n",
    "        \n",
    "        output_comp = model_comp(data_input_1.narrow(0, b, mini_batch_size), data_input_2.narrow(0, b, mini_batch_size),train=False)\n",
    "       \n",
    "        output_comp = torch.round(output_comp)\n",
    "       \n",
    "       \n",
    "        for k in range(mini_batch_size):\n",
    "            #print(torch.eq(data_target[b + k], output_comp[k]))\n",
    "            if torch.equal(torch.eq(data_target[b + k], output_comp[k]),torch.tensor([True,True])) == False:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_siamese(model_digit, model_comp, tr_input_1, tr_input_2, tr_target, te_input_1, te_input_2, te_target):\n",
    "    print('train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "                compute_nb_errors_siamese(model_digit, model_comp, tr_input_1, tr_input_2, tr_target) / N * 100,\n",
    "                compute_nb_errors_siamese(model_digit, model_comp, te_input_1, te_input_2, te_target) / N * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error 0.30% test_error 8.00%\n"
     ]
    }
   ],
   "source": [
    "print_error_siamese(model_digit, model_comp, train_input_1, train_input_2, train_target, test_input_1, test_input_2, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
