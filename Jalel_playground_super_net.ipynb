{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(Tensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9, 3],\n",
       "        [5, 4],\n",
       "        [7, 4],\n",
       "        [9, 6],\n",
       "        [8, 8]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000 # Number of data samples in training and test set\n",
    "\n",
    "train_input, train_target, train_classes, \\\n",
    "    test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(train_classes.shape)\n",
    "\n",
    "train_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input, mean, std):\n",
    "    input.sub_(mean).div_(std)\n",
    "    \n",
    "def process_data(img_input, classes, one_hot_classes=False):\n",
    "    \n",
    "    n_img = img_input.size(0) \n",
    "    img_input_1 = img_input[:,0,:,:].reshape(n_img, 1, 14, 14)\n",
    "    img_input_2 = img_input[:,1,:,:].reshape(n_img, 1, 14, 14)\n",
    "    \n",
    "    img_classes_1 = prologue.convert_to_one_hot_labels(img_input_1, classes[:,0]) if one_hot_classes else classes[:,0]\n",
    "    img_classes_2 = prologue.convert_to_one_hot_labels(img_input_2, classes[:,1]) if one_hot_classes else classes[:,1]\n",
    "    \n",
    "    img_classes_1.reshape(-1,1)\n",
    "    img_classes_2.reshape(-1,1)\n",
    "    \n",
    "    return img_input_1, img_input_2, img_classes_1, img_classes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_input.mean(dim=(0,2,3), keepdim=True)\n",
    "std = train_input.std(dim=(0,2,3), keepdim=True)\n",
    "\n",
    "normalize(train_input, mean, std)\n",
    "normalize(test_input, mean, std)\n",
    "\n",
    "train_input_1, train_input_2, train_classes_1, train_classes_2 = process_data(train_input, train_classes)\n",
    "test_input_1, test_input_2, test_classes_1, test_classes_2 = process_data(test_input, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(DigitNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompNet(torch.nn.Module):\n",
    "    def __init__(self, digitNet):\n",
    "        super(CompNet, self).__init__()\n",
    "        self.digitNet = digitNet\n",
    "        self.fc1 = nn.Linear(20, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "        \n",
    "    def forward(self, x1, x2,train=True):\n",
    "        x1 = self.digitNet.forward(x1)\n",
    "        x2 = self.digitNet.forward(x2)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x,p=0.25,training=train)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x,p=0.25,training=train)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_digit, model_comp, \n",
    "                train_input_1, train_input_2, train_classes_1, train_classes_2, train_target, \n",
    "                criterion_digit=nn.CrossEntropyLoss(), criterion_comp=nn.BCELoss(), \n",
    "                mini_batch_size=25,nb_epochs=50, lr=1e-1):\n",
    "    \n",
    "    #optimizer_digit = torch.optim.SGD(model_digit.parameters(), lr=lr)\n",
    "    optimizer_comp = torch.optim.SGD(model_comp.parameters(), lr=lr)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        if e % 5 == 0:\n",
    "            print(\"Epochs {}\".format(e))\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            # digit classification \n",
    "            output_img_1 = model_digit(train_input_1.narrow(0, b, mini_batch_size))\n",
    "            output_img_2 = model_digit(train_input_2.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            loss_img_1 = criterion_digit(output_img_1, train_classes_1.narrow(0, b, mini_batch_size))\n",
    "            loss_img_2 = criterion_digit(output_img_2, train_classes_2.narrow(0, b, mini_batch_size))\n",
    "            loss_img = loss_img_1 + loss_img_2\n",
    "            \n",
    "            output_comp = model_comp(train_input_1.narrow(0, b, mini_batch_size), train_input_2.narrow(0, b, mini_batch_size))\n",
    "            batch_target = train_target.narrow(0, b, mini_batch_size).reshape(-1,1).float()\n",
    "            loss_comp = criterion_comp(output_comp, batch_target)\n",
    "            \n",
    "            loss = loss_img + loss_comp\n",
    "            \n",
    "            if b==0:\n",
    "                print(\"loss = {}, loss_img = {}, loss_comp = {}\".format(loss, loss_img, loss_comp))\n",
    "                \n",
    "            model_digit.zero_grad()\n",
    "            model_comp.zero_grad()\n",
    "            loss.backward()\n",
    "            #optimizer_digit.step()\n",
    "            optimizer_comp.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152326\n",
      "155977\n",
      "training...\n",
      "Epochs 0\n",
      "loss = 5.319052219390869, loss_img = 4.6093292236328125, loss_comp = 0.7097231149673462\n",
      "loss = 2.803295850753784, loss_img = 2.108490228652954, loss_comp = 0.6948056817054749\n",
      "loss = 1.9616011381149292, loss_img = 1.333293080329895, loss_comp = 0.6283080577850342\n",
      "loss = 1.6165674924850464, loss_img = 0.9138633012771606, loss_comp = 0.7027041912078857\n",
      "loss = 1.2393767833709717, loss_img = 0.607751727104187, loss_comp = 0.6316249966621399\n",
      "Epochs 5\n",
      "loss = 1.0401082038879395, loss_img = 0.3801381587982178, loss_comp = 0.6599700450897217\n",
      "loss = 0.7727312445640564, loss_img = 0.20695598423480988, loss_comp = 0.5657752752304077\n",
      "loss = 0.8352118730545044, loss_img = 0.18276508152484894, loss_comp = 0.6524468064308167\n",
      "loss = 0.6467736959457397, loss_img = 0.07476088404655457, loss_comp = 0.5720128417015076\n",
      "loss = 0.6745926141738892, loss_img = 0.05755317956209183, loss_comp = 0.6170394420623779\n",
      "Epochs 10\n",
      "loss = 0.5492487549781799, loss_img = 0.031715862452983856, loss_comp = 0.5175328850746155\n",
      "loss = 0.7794485092163086, loss_img = 0.04338961839675903, loss_comp = 0.7360588908195496\n",
      "loss = 0.5181742310523987, loss_img = 0.04480506852269173, loss_comp = 0.47336915135383606\n",
      "loss = 0.5746660828590393, loss_img = 0.020007967948913574, loss_comp = 0.5546581149101257\n",
      "loss = 0.4888652265071869, loss_img = 0.012425331398844719, loss_comp = 0.476439893245697\n",
      "Epochs 15\n",
      "loss = 0.6819490790367126, loss_img = 0.027778830379247665, loss_comp = 0.6541702747344971\n",
      "loss = 0.49401339888572693, loss_img = 0.009049797430634499, loss_comp = 0.484963595867157\n",
      "loss = 0.5155994892120361, loss_img = 0.01099464576691389, loss_comp = 0.5046048164367676\n",
      "loss = 0.48521241545677185, loss_img = 0.010166415013372898, loss_comp = 0.4750460088253021\n",
      "loss = 0.46745190024375916, loss_img = 0.008054167032241821, loss_comp = 0.45939773321151733\n",
      "Epochs 20\n",
      "loss = 0.5093949437141418, loss_img = 0.03457742556929588, loss_comp = 0.47481751441955566\n",
      "loss = 0.45433518290519714, loss_img = 0.007183077745139599, loss_comp = 0.44715210795402527\n",
      "loss = 0.5144163370132446, loss_img = 0.003165151923894882, loss_comp = 0.5112512111663818\n",
      "loss = 0.45796918869018555, loss_img = 0.01075422391295433, loss_comp = 0.4472149610519409\n",
      "loss = 0.5447835326194763, loss_img = 0.06501086056232452, loss_comp = 0.4797726571559906\n",
      "Epochs 25\n",
      "loss = 0.5025602579116821, loss_img = 0.007761489599943161, loss_comp = 0.4947987496852875\n",
      "loss = 0.47587916254997253, loss_img = 0.015485481359064579, loss_comp = 0.46039366722106934\n",
      "loss = 0.4861866235733032, loss_img = 0.012535507790744305, loss_comp = 0.47365111112594604\n",
      "loss = 0.48340898752212524, loss_img = 0.009474973194301128, loss_comp = 0.47393402457237244\n",
      "loss = 0.44759687781333923, loss_img = 0.00355253042653203, loss_comp = 0.4440443515777588\n",
      "Epochs 30\n",
      "loss = 0.46100297570228577, loss_img = 0.0016503302613273263, loss_comp = 0.45935264229774475\n",
      "loss = 0.4535197913646698, loss_img = 0.0022193382028490305, loss_comp = 0.4513004422187805\n",
      "loss = 0.4489126205444336, loss_img = 0.0052466075867414474, loss_comp = 0.443666011095047\n",
      "loss = 0.4454178214073181, loss_img = 0.0016344728646799922, loss_comp = 0.44378334283828735\n",
      "loss = 0.4465413987636566, loss_img = 0.002918562851846218, loss_comp = 0.4436228275299072\n",
      "Epochs 35\n",
      "loss = 0.44716572761535645, loss_img = 0.0018826136365532875, loss_comp = 0.44528311491012573\n",
      "loss = 0.5129793286323547, loss_img = 0.013047678396105766, loss_comp = 0.4999316334724426\n",
      "loss = 0.44564884901046753, loss_img = 0.0020154155790805817, loss_comp = 0.44363343715667725\n",
      "loss = 0.44815492630004883, loss_img = 0.0045179277658462524, loss_comp = 0.44363701343536377\n",
      "loss = 0.44495028257369995, loss_img = 0.0013315848773345351, loss_comp = 0.44361868500709534\n",
      "Epochs 40\n",
      "loss = 0.4457947611808777, loss_img = 0.0021803248673677444, loss_comp = 0.4436144232749939\n",
      "loss = 0.44654059410095215, loss_img = 0.002919366117566824, loss_comp = 0.4436212182044983\n",
      "loss = 0.47312599420547485, loss_img = 0.0017851178999990225, loss_comp = 0.4713408648967743\n",
      "loss = 0.4504554271697998, loss_img = 0.001987108727917075, loss_comp = 0.44846832752227783\n",
      "loss = 0.5383502244949341, loss_img = 0.07601212710142136, loss_comp = 0.46233808994293213\n",
      "Epochs 45\n",
      "loss = 0.4448951184749603, loss_img = 0.0008630550000816584, loss_comp = 0.44403207302093506\n",
      "loss = 0.44783711433410645, loss_img = 0.004022795706987381, loss_comp = 0.44381430745124817\n",
      "loss = 0.4441835284233093, loss_img = 0.0005662707844749093, loss_comp = 0.4436172544956207\n",
      "loss = 0.44485291838645935, loss_img = 0.0012358003295958042, loss_comp = 0.4436171054840088\n"
     ]
    }
   ],
   "source": [
    "model_digit = DigitNet(500)\n",
    "model_comp = CompNet(model_digit)\n",
    "\n",
    "print(sum(p.numel() for p in model_digit.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in model_comp.parameters() if p.requires_grad))\n",
    "print(\"training...\")\n",
    "\n",
    "train_model(model_digit=model_digit, model_comp=model_comp,\n",
    "            train_input_1=train_input_1, train_input_2=train_input_2,\n",
    "            train_classes_1=train_classes_1, train_classes_2=train_classes_2, \n",
    "            train_target=train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model_digit, model_comp,\n",
    "                              data_input_1, data_input_2, data_target, mini_batch_size=25):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input_1.size(0), mini_batch_size):\n",
    "        #output_img_1 = model_digit(data_input_1.narrow(0, b, mini_batch_size))\n",
    "        #output_img_2 = model_digit(data_input_2.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        output_comp = model_comp(data_input_1.narrow(0, b, mini_batch_size), data_input_2.narrow(0, b, mini_batch_size),train=False)\n",
    "        output_comp = torch.round(output_comp)\n",
    "    \n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != output_comp[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_siamese(model_digit, model_comp, tr_input_1, tr_input_2, tr_target, te_input_1, te_input_2, te_target):\n",
    "    print('train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "                compute_nb_errors_siamese(model_digit, model_comp, tr_input_1, tr_input_2, tr_target) / N * 100,\n",
    "                compute_nb_errors_siamese(model_digit, model_comp, te_input_1, te_input_2, te_target) / N * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error 3.10% test_error 10.30%\n"
     ]
    }
   ],
   "source": [
    "print_error_siamese(model_digit, model_comp, train_input_1, train_input_2, train_target, test_input_1, test_input_2, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
